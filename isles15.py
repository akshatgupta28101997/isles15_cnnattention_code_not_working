# -*- coding: utf-8 -*-
"""isles15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bmmHR531GjbfilIZkTSKKF-Btakuuj4c
"""

from google.colab import drive
drive.mount('/content/drive')

from __future__ import print_function
from keras.preprocessing.image import ImageDataGenerator
import numpy as np 
import os
import glob
import skimage.io as io
import skimage.transform as trans
from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
from keras import backend as keras
import cv2

# /content/drive/My Drive/final_images_modified_renumbered.zip
!unzip -q "/content/drive/My Drive/siss - Copy.zip"

a = []
k=256
path = "/content/siss - Copy/"
for i in range(1):
   print(i)
   im1 = cv2.imread(path + "Flair/"+ str(i) + ".jpg",0)
   im1 = cv2.resize(im1,(k,k))
   im1=im1/255
   im1 = np.expand_dims(im1,axis=2)

#
  #  im2 = cv2.imread(path + "DWI/"+ str(i) + ".jpg",0)
  #  im2 = cv2.resize(im2,(k,k))
  #  im2=im2/255
  #  im2 = np.expand_dims(im2,axis=2)
   
  #  im3 = cv2.imread(path + "T1/"+ str(i) + ".jpg",0)
  #  im3 = cv2.resize(im3,(k,k))
  #  im3=im3/255
  #  im3 = np.expand_dims(im3,axis=2)
  
  
  #  im4 = cv2.imread(path+"T2/"+ str(i) + ".jpg",0)
  #  im4 = cv2.resize(im4,(k,k))
  #  im4 = im4/255
  #  im4= np.expand_dims(im4,axis=2)
   
  #  ans = np.concatenate((im1,im2,im3,im4),axis=2)

   ans = np.expand_dims(im1,axis=0)
   
   
   print(ans.shape)

for i in range(1,697,1):
   print(i)
   im1 = cv2.imread(path + "Flair/"+ str(i) + ".jpg",0)
   im1 = cv2.resize(im1,(k,k))
   im1=im1/255
   im1 = np.expand_dims(im1,axis=2)


  #  im2 = cv2.imread(path + "DWI/"+ str(i) + ".jpg",0)
  #  im2 = cv2.resize(im2,(k,k))
  #  im2=im2/255
  #  im2 = np.expand_dims(im2,axis=2)
   
  #  im3 = cv2.imread(path + "T1/"+ str(i) + ".jpg",0)
  #  im3 = cv2.resize(im3,(k,k))
  #  im3=im3/255
  #  im3 = np.expand_dims(im3,axis=2)
  
  
  #  im4 = cv2.imread(path+"T2/"+ str(i) + ".jpg",0)
  #  im4 = cv2.resize(im4,(k,k))
  #  im4 = im4/255
  #  im4= np.expand_dims(im4,axis=2)
   
   
   
   
   
  #  ans2 = np.concatenate((im1,im2,im3,im4),axis=2)
   ans2 = np.expand_dims(im1,axis=0)
  
   
   ans = np.concatenate((ans,ans2),axis=0)

ans.shape

#%% preparing labels
   
   
   
   
for i in range(1):
    print(i)
    im1 = cv2.imread(path + "OT/"+str(i)+".jpg",0)
    
    im1 = cv2.resize(im1,(k,k))
    for i in range(k):
      for j in range(k):
        if(im1[i][j]>128):
          im1[i][j]=1
        else:
          im1[i][j]=0
          
   
    print(np.unique(im1))
    
    im1 = np.expand_dims(im1,axis=2)
    labels = np.expand_dims(im1,axis=0)
   
for i in range(1,697,1):
    print(i)
    im1 = cv2.imread(path + "OT/"+str(i)+".jpg",0)
    im1 = cv2.resize(im1,(k,k))
    for i in range(k):
      for j in range(k):
        if(im1[i][j]>128):
          im1[i][j]=1
        else:
          im1[i][j]=0
   
    im1 = np.expand_dims(im1,axis=0)
    im1 = np.expand_dims(im1,axis=3)
    labels = np.concatenate((labels,im1),axis=0)

print(ans.shape)
print(labels.shape)

from keras.layers import Layer
from keras.layers import Conv2D
from keras.layers import concatenate
from keras import initializers
from keras import backend as K
import tensorflow as tf

np.unique(ans)

ans = ans.astype(np.float16)
labels = labels.astype(np.float16)

np.unique(ans)

from keras.layers import Conv2D
from keras.layers import concatenate

from keras import backend as K
from keras.layers import LeakyReLU,Lambda,Input,Convolution2D,UpSampling2D,Dropout,BatchNormalization,MaxPooling2D
from keras.models import Model

def _conv_layer(filters, kernel_size):
    return Conv2D(filters, kernel_size, padding='same',
                  use_bias=True, kernel_initializer='he_normal')



def augmented_conv2d(ip, filters, kernel_size=(3, 3), strides=(1, 1),
                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):
    
   

    
    
    conv1 = _conv_layer(int(filters/8),3)(ip)
    conv1 = LeakyReLU(alpha=0.2)(conv1)
    conv1 = Lambda(lambda x: K.mean(x,axis=3))(conv1)
    conv1 = Lambda(lambda x: K.expand_dims(x,axis=-1))(conv1)
    conv1 = Lambda(lambda x:K.repeat_elements(x,rep = int(filters/8),axis=-1))(conv1)
    
    conv2 = _conv_layer(int(filters/8),3)(ip)
    conv2 = LeakyReLU(alpha=0.2)(conv2)
    conv2 = Lambda(lambda x: K.mean(x,axis=3))(conv2)
    conv2 = Lambda(lambda x: K.expand_dims(x,axis=-1))(conv2)
    conv2 = Lambda(lambda x:K.repeat_elements(x,rep = int(filters/8),axis=-1))(conv2)
    
    conv3 = _conv_layer(int(filters/8),3)(ip)
    conv3 = LeakyReLU(alpha=0.2)(conv3)
    conv3 = Lambda(lambda x: K.mean(x,axis=3))(conv3)
    conv3 = Lambda(lambda x: K.expand_dims(x,axis=-1))(conv3)
    conv3 = Lambda(lambda x:K.repeat_elements(x,rep = int(filters/8),axis=-1))(conv3)
    
    
    conv4 = _conv_layer(int(filters/8),3)(ip)
    conv4 = LeakyReLU(alpha=0.2)(conv4)
    conv4 = Lambda(lambda x: K.mean(x,axis=3))(conv4)
    conv4 = Lambda(lambda x: K.expand_dims(x,axis=-1))(conv4)
    conv4 = Lambda(lambda x:K.repeat_elements(x,rep = int(filters/8),axis=-1))(conv4)
    
    concat = concatenate([conv1,conv2,conv3,conv4],axis=3)
    path2 = _conv_layer(int(filters/2),3)(ip)
    final_out  = concatenate([concat,path2],axis=3)
    return final_out

input_size = (256,256,1)
inputs = Input(input_size) #returns a tensor
#conv1 = Convolution2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
conv1 = augmented_conv2d(inputs,  filters=32 )
conv1 = LeakyReLU(alpha=0.2)(conv1)
conv1 = BatchNormalization()(conv1)
conv1 =augmented_conv2d( conv1, filters=32 )
conv1 = LeakyReLU(alpha=0.2)(conv1)
conv1 = BatchNormalization()(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = augmented_conv2d( pool1, filters=64 )
conv2= LeakyReLU(alpha=0.2)(conv2)
conv2 = BatchNormalization()(conv2)
conv2 = augmented_conv2d(conv2,  filters=64 )
conv2 = LeakyReLU(alpha=0.2)(conv2)
conv2 = BatchNormalization()(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
conv3 = augmented_conv2d(pool2,  filters=128 )
conv3 = LeakyReLU(alpha=0.2)(conv3)
conv3 = BatchNormalization()(conv3)
conv3 = augmented_conv2d(conv3,  filters=128 )
conv3 = LeakyReLU(alpha=0.2)(conv3)
conv3 = BatchNormalization()(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
conv4 = augmented_conv2d(pool3,  filters=256 )
conv4 = LeakyReLU(alpha=0.2)(conv4)
conv4 = BatchNormalization()(conv4)
conv4 = augmented_conv2d(conv4,  filters=256 )
conv4 = BatchNormalization()(conv4)
drop4 = Dropout(0.5)(conv4)
pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)
#    
conv5 = augmented_conv2d( pool4, filters=512 )
conv5 = LeakyReLU(alpha=0.2)(conv5)
# conv5 = BatchNormalization()(conv5)
conv5 = augmented_conv2d(conv5,  filters=512 )
conv5 = LeakyReLU(alpha=0.2)(conv5)
# conv5 = BatchNormalization()(conv5)
drop5 = Dropout(0.5)(conv5)
#    
up6 = augmented_conv2d(UpSampling2D(size = (2,2))(drop5),  filters=256 )
up6 = LeakyReLU(alpha=0.2)(up6)
merge6 = concatenate([drop4,up6],axis=3)
conv6 = augmented_conv2d( merge6, filters=256 )
conv6 = LeakyReLU(alpha=0.2)(conv6)
# conv6 = BatchNormalization()(conv6)
conv6 = augmented_conv2d(conv6,  filters=256 )
conv6 = LeakyReLU(alpha=0.2)(conv6)
# conv6 = BatchNormalization()(conv6)
   
up7 = augmented_conv2d(UpSampling2D(size = (2,2))(conv6),  filters=128 )
up7 = LeakyReLU(alpha=0.2)(up7)
merge7 = concatenate([conv3,up7], axis = 3)
conv7 = augmented_conv2d(merge7,  filters=128 )
conv7 = LeakyReLU(alpha=0.2)(conv7)
# conv7 = BatchNormalization()(conv7)
conv7 = augmented_conv2d(conv7,  filters=128 )
conv7 = LeakyReLU(alpha=0.2)(conv7)
# conv7 = BatchNormalization()(conv7)

up8 = augmented_conv2d(UpSampling2D(size = (2,2))(conv7),  filters=64 )
up8 = LeakyReLU(alpha=0.2)(up8)
merge8 = concatenate([conv2,up8], axis = 3)
conv8 = augmented_conv2d( merge8, filters=64 )
conv8 = LeakyReLU(alpha=0.2)(conv8)
# conv8 = BatchNormalization()(conv8)
conv8 = augmented_conv2d(conv8,  filters=64 )
conv8 = LeakyReLU(alpha=0.2)(conv8)
# conv8 = BatchNormalization()(conv8)
#    
up9 = augmented_conv2d( UpSampling2D(size = (2,2))(conv8), filters=32 )
up9 = LeakyReLU(alpha=0.2)(up9)
merge9 = concatenate([conv1,up9],axis = 3)
conv9 = augmented_conv2d(merge9 , filters=32 )
conv9 = LeakyReLU(alpha=0.2)(conv9)
# conv9 = BatchNormalization()(conv9)
conv9 = augmented_conv2d(conv9,  filters=32 )
conv9 = LeakyReLU(alpha=0.2)(conv9)
# conv9 = BatchNormalization()(conv9)


conv10 = Convolution2D(2,3,padding='same')(conv9)
conv10 = LeakyReLU(alpha=0.2)(conv10)
# conv10 = BatchNormalization()(conv10)

conv11 = Convolution2D(1,1,padding='same',activation = 'sigmoid')(conv10)

model = Model(input = inputs, output = conv11)
model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])

   
print(model.summary())

fold=2
rm = np.arange(70*(fold-1),70*fold,1)
ans2 = np.delete(ans,rm,axis=0)
labels2 = np.delete(labels,rm,axis=0)
test2 = ans[70*fold-70:70*fold]

print(ans2.shape)
print(labels2.shape)
print(test2.shape)

fold

print(labels2.dtype)

callbacks_list= [EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)]
hist = model.fit(ans2[:600],labels2[:600],epochs=30,batch_size=8,callbacks=callbacks_list,validation_data=[ans2[600:],labels2[600:]],shuffle=True)

labels2.shape

print(tf.__version__)

import matplotlib.pyplot as plt
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

fold

from google.colab import drive
drive.mount('/content/drive')
import pickle

# save:
f = open('/content/drive/My Drive/15_flair_att'+str(fold)+'.pckl', 'wb')
pickle.dump(hist.history, f)
f.close()

# retrieve:    
f = open('/content/drive/My Drive/history_spes_10.pckl', 'rb')
history = pickle.load(f)
f.close()

from google.colab import drive
drive.mount('/content/drive')
import pickle

print(ans.shape)
print(labels.shape)
print(test2.shape)



from google.colab import drive
drive.mount('/content/drive')

predd = model.predict(test2)
predd.shape

p=70
for i in range(p):
  print(i)
  io.imsave("/content/drive/My Drive/isles_15/"+str(i)+"_coreflairatt"+str(fold)+".jpg",predd[i])

print(p)
print(k)

#dice
import cv2
import numpy as np
result = []
for l in range(p):
    print(l)
    
    im1 = cv2.imread("/content/drive/My Drive/isles_15"  +'/'+str(l) + '_coreflairatt'+str(fold)+'.jpg',0)

    im1 = cv2.resize(im1,(k,k))
        
    ret,pred = cv2.threshold(im1,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#    print(ret)
    
#    cv2.imshow("thresholded prediction",pred)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
    t = 70*(fold-1) +l
    im2 = cv2.imread(path + "OT/"+str(t)+".jpg",0)
    im2 = cv2.resize(im2,(k,k))
    
    
    
    
     
  
#    cv2.drawContours(input_im, contours, -1, (255,0,0), 2)
    
#
#    cv2.imshow("thresholded prediction",pred)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
#    
    
    ret,label = cv2.threshold(im2,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#    cv2.imshow("thresholded label",label)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
    #
    count=0
    for i in range (k):
        for j in range (k):
            if pred[i][j] ==label[i][j] and pred[i][j]==255:
                count+=1
    den1 = np.count_nonzero(pred)
    den2 = np.count_nonzero(label)
    den = den1+den2
    anss = 2*count/(den)
    result= np.append(result,anss)
print('flair')    
print(np.average(result)) 
print(np.std(result))

result

# for flair
print('sensitivity')
import cv2
import numpy as np
result = []

for l in range(p):
    print(l)
    im1 = cv2.imread("/content/drive/My Drive/isles_15"  +'/'+str(l) + '_coredwicorrectd'+str(fold)+'.jpg',0)

    t = 70*(fold-1) +l
    im2 = cv2.imread(path + "OT/"+str(t)+".jpg",0)
    
    im2 = cv2.resize(im2,(k,k))
    
    ret,pred = cv2.threshold(im1,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#    cv2.imshow("thresholded prediction",pred)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
    
    
    ret,label = cv2.threshold(im2,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#    cv2.imshow("thresholded label",label)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
    #
    tp=0
    fn = 0
    for i in range (k):
        for j in range (k):
            if pred[i][j] ==label[i][j] and pred[i][j]==255:
                tp+=1
            if pred[i][j]==0  and label[i][j]==255:
                fn+=1
                
    
    anss = tp/(tp+fn+1)
    result= np.append(result,anss)
print('flair')    
print(np.average(result))  
print(np.std(result))

from google.colab import drive
drive.mount('/content/drive')

# for flair
print('specificity')
import cv2
import numpy as np
result = []

for l in range(p):
    print(l)
    im1 = cv2.imread("/content/drive/My Drive/isles_15"  +'/'+str(l) + '_coredwicorrectd'+str(fold)+'.jpg',0)

    t = 70*(fold-1) +l
    im2 = cv2.imread(path + "OT/"+str(t)+".jpg",0)
    im2 = cv2.resize(im2,(k,k))
    
    ret,pred = cv2.threshold(im1,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#    cv2.imshow("thresholded prediction",pred)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
    
    
    ret,label = cv2.threshold(im2,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#    cv2.imshow("thresholded label",label)
#    cv2.waitKey()
#    cv2.destroyAllWindows()
    #
    tn=0
    fp = 0
    for i in range (k):
        for j in range (k):
            if pred[i][j]==0  and label[i][j]==0:
                tn+=1
            if pred[i][j]==255  and label[i][j]==0:
                fp+=1
                
    
    anss = tn/(tn+fp)
    result= np.append(result,anss)
print('flair')
print(np.average(result))  
print(np.std(result))

p

fold

k

ad